{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fine tuning our model\n",
    "Spot checking algorithms helped us identify a candidate to model our issue. In our current projet, it's a random forest algorithm. We will now try to fine tune this type of model.\n",
    "\n",
    "What method is best suited? This article on __[Hyperparameter Tuning the Random Forest in Python](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)__ provides a good methodology that we follow here, as it allows for:\n",
    "1. first quick browsing hyperparameter range across a Random Hyperparameter Grid. This helps narrow down our research for the best hyperparameters. \n",
    "2. then focus on a smaller grid where all combinations of the specified hyperparameters will be tested.\n",
    "\n",
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 414 entries, 0 to 413\n",
      "Data columns (total 8 columns):\n",
      "No                                        414 non-null int64\n",
      "X1 transaction date                       414 non-null float64\n",
      "X2 house age                              414 non-null float64\n",
      "X3 distance to the nearest MRT station    414 non-null float64\n",
      "X4 number of convenience stores           414 non-null int64\n",
      "X5 latitude                               414 non-null float64\n",
      "X6 longitude                              414 non-null float64\n",
      "Y house price of unit area                414 non-null float64\n",
      "dtypes: float64(6), int64(2)\n",
      "memory usage: 26.0 KB\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "path = \" \"\n",
    "df = pd.read_excel(path+\"Real estate valuation data set.xlsx\")\n",
    "df.info()\n",
    "\n",
    "df.set_index('No', inplace = True)\n",
    "\n",
    "# Define the variables\n",
    "X = df.drop(['Y house price of unit area','X6 longitude'], axis =1)\n",
    "y = df['Y house price of unit area'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/utils/fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  _nan_object_mask = _nan_object_array != _nan_object_array\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "//anaconda/lib/python3.5/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "# Normalise and scale the numerical features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "ss_X = StandardScaler()\n",
    "X_train = ss_X.fit_transform(X_train)\n",
    "mms_X = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train = mms_X.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:2: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Normalise and scale the numerical features - X_test\n",
    "X_test = ss_X.transform(X_test)\n",
    "X_test = mms_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "First, we look at the default parameters of our random forest model and its performance. We will use it as our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'criterion': 'mse',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 10,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': None,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regressor = RandomForestRegressor()\n",
    "\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "from pprint import pprint\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(regressor.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a function for model evaluation using cross validation\n",
    "def evaluate_model_cross_validation(name, model, X_train, y_train, folds = 10):\n",
    "\n",
    "    from sklearn.model_selection import cross_val_score\n",
    " \n",
    "    # Cross Validation Regression MAE\n",
    "    metric='neg_mean_absolute_error'\n",
    "    scores = cross_val_score(regressor, X_train, y_train, scoring=metric, cv=folds, n_jobs=-1)\n",
    "    mean_score, std_score = np.mean(scores), np.std(scores)\n",
    "    print('>%s - training - MAE: %.3f (+/-%.3f)' % (name, mean_score, std_score))\n",
    "    \n",
    "    \n",
    "    # Cross Validation Regression MSE\n",
    "    metric='neg_mean_squared_error'\n",
    "    scores = cross_val_score(regressor, X_train, y_train, scoring=metric, cv=folds, n_jobs=-1)\n",
    "    mean_score, std_score = np.mean(scores), np.std(scores)\n",
    "    print('>%s - training - MSE: %.3f (+/-%.3f)' % (name, mean_score, std_score))\n",
    "    \n",
    "    # Cross Validation Regression R^2\n",
    "    metric='r2'\n",
    "    scores = cross_val_score(regressor, X_train, y_train, scoring=metric, cv=folds, n_jobs=-1)\n",
    "    mean_score, std_score = np.mean(scores), np.std(scores)\n",
    "    print('>%s - training - R^2: %.3f (+/-%.3f)' % (name, mean_score, std_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a function for model evaluation using a test set\n",
    "def evaluate_model_test_set(name, model, y_test, y_predicted):\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    print('>%s - test - MAE: %.3f' % (name, mean_absolute_error(y_test, y_predicted)))\n",
    "    print('>%s - test - MSE: %.3f' % (name, mean_squared_error(y_test, y_predicted)))\n",
    "    print('>%s - test - R^2: %.3f' % (name, r2_score(y_test, y_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">RF_baseline - training - MAE: -5.030 (+/-0.939)\n",
      ">RF_baseline - training - MSE: -64.689 (+/-59.342)\n",
      ">RF_baseline - training - R^2: 0.655 (+/-0.161)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the baseline model\n",
    "evaluate_model_cross_validation(\"RF_baseline\", regressor, X_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">RF_baseline - test - MAE: 5.207\n",
      ">RF_baseline - test - MSE: 64.513\n",
      ">RF_baseline - test - R^2: 0.621\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_test_set(\"RF_baseline\", regressor, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search Cross Validation\n",
    "We will first perform random search training to try out a wide range of values and see what works!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the random grid\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n"
     ]
    }
   ],
   "source": [
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the article cited above: \"The most important arguments in RandomizedSearchCV are <b>n_iter</b>, which controls \n",
    "the number of different combinations to try, and <b>cv</b> which is the number of folds\n",
    " to use for cross validation (we use 100 and 3 respectively). More iterations will\n",
    " cover a wider search space and more cv folds reduces the chances of overfitting,\n",
    " but raising each will increase the run time. Machine learning is a field of trade-offs,\n",
    " and performance vs time is one of the most fundamental.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First create the base model to tune \n",
    "regressor = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rf_random = RandomizedSearchCV(estimator = regressor,\n",
    "                               param_distributions = random_grid,\n",
    "                               n_iter = 100, cv = 3, verbose=2, random_state=42,\n",
    "                               n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  2.7min finished\n",
      "//anaconda/lib/python3.5/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'bootstrap': [True, False], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_leaf': [1, 2, 4], 'min_samples_split': [2, 5, 10]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the random search model (This might take a while)\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 110,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 1000}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the best parameters\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting Random forest Model to the dataset using the best parameters after random search\n",
    "rf_r = RandomForestRegressor(bootstrap = True,\n",
    "                                  max_depth = 110,\n",
    "                                  max_features ='sqrt',\n",
    "                                  min_samples_leaf = 1,\n",
    "                                  min_samples_split = 2,\n",
    "                                  n_estimators = 1000)\n",
    "\n",
    "rf_r.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = rf_r.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">RF_random - training - MAE: -5.280 (+/-1.113)\n",
      ">RF_random - training - MSE: -65.284 (+/-55.941)\n",
      ">RF_random - training - R^2: 0.704 (+/-0.137)\n"
     ]
    }
   ],
   "source": [
    "# evaluate model after random grid search\n",
    "evaluate_model_cross_validation(\"RF_random\", rf_r, X_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">RF_random - test - MAE: 4.563\n",
      ">RF_random - test - MSE: 51.341\n",
      ">RF_random - test - R^2: 0.699\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_test_set(\"RF_random\", rf_r, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our metrics improved from our baseline model. Let's check if we can further improve them by fine tuning our model using a systematic grid search.\n",
    "## Grid Search with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Applying Grid Search to find the best model and the best parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [800, 900, 1000, 1100, 1200]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [100, 110, 120]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 3]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True]\n",
    "# Create the random grid\n",
    "parameters = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First create the base model to tune \n",
    "regressor = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator = regressor,\n",
    "                           param_grid = parameters,\n",
    "                           cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 5 x 1 x 3 x 2 x 2 x 2 x 1 = 60 features and 3 folds, which makes 180 combinations of settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 60 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   36.4s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:   47.1s finished\n",
      "//anaconda/lib/python3.5/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Fit the grid search model (This might take a while)\n",
    "grid_search = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 120,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 3,\n",
       " 'n_estimators': 900}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the best parameters\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting Random forest Model to the dataset using the best parameters\n",
    "rf_g = RandomForestRegressor(bootstrap = True,\n",
    "                                  max_depth = 120,\n",
    "                                  max_features ='sqrt',\n",
    "                                  min_samples_leaf = 1,\n",
    "                                  min_samples_split = 3,\n",
    "                                  n_estimators = 900)\n",
    "\n",
    "rf_g.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = rf_g.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">RF_grid_search - training - MAE: -4.994 (+/-1.043)\n",
      ">RF_grid_search - training - MSE: -65.844 (+/-61.745)\n",
      ">RF_grid_search - training - R^2: 0.691 (+/-0.107)\n"
     ]
    }
   ],
   "source": [
    "# evaluate model after grid search\n",
    "evaluate_model_cross_validation(\"RF_grid_search\", rf_g, X_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">RF_grid_search - test - MAE: 4.587\n",
      ">RF_grid_search - test - MSE: 51.736\n",
      ">RF_grid_search - test - R^2: 0.696\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_test_set(\"RF_grid_search\", rf_g, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The new random forest model after a systematic grid search does not yield better performances than the random forest model selected after a randomized grid search. \n",
    "# Conclusion\n",
    "We have found a model that help us predict the house price of unit area in Xindian district using a few features. Through this project, we have seen how to:\n",
    "* Conduct an exploratory data analysis using Python and Tableau.\n",
    "* Build summary maps using Graphic Information Processing.\n",
    "* Test rapidly different regression models using spot checking.\n",
    "* Fine tune our regression candidate using Random search and then Grid search.\n",
    "\n",
    "Further analysis could include:\n",
    "* Excluding some of the properties identified as outliers in the exploratory phase.\n",
    "* Run a few additional randomized grid search to identify hyper parameters that increase the performances of the random forest model.\n",
    "* Perform some clustering analysis to assess if we can identify geographical categories, as those identified with Graphic Information Processing.\n",
    "* Testing the model with more recent data to check how the market has evolved and the need for building a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
